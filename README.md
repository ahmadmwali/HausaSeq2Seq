# Automatic Correction of Writing Anomalies in Hausa Texts

This repository contains the official code and resources for the research paper "Automatic Correction of Writing Anomalies in Hausa Texts". The project focuses on developing robust systems for identifying and correcting various types of errors in Hausa text, a low-resource language. We introduce a novel synthetic noise generation pipeline and fine-tune several state-of-the-art transformer models to achieve effective text correction.

## Key Features

- **Synthetic Data Generation**: A sophisticated pipeline (`get_data.py`) to create large-scale, parallel noisy-clean text corpora by simulating realistic writing anomalies found in Hausa.

- **Transformer Model Fine-tuning**: Scripts (`train.py`) to fine-tune powerful sequence-to-sequence models like M2M100, mBART, and AfriTEVA for the text correction task, with support for full fine-tuning and Parameter-Efficient Fine-Tuning (PEFT) using LoRA.

- **Comprehensive Evaluation**: Robust testing (`test.py`) using multiple automatic metrics (BLEU, WER, CER, METEOR) and evaluation on a downstream sentiment analysis task (`evaluate_downstream.py`) to measure the real-world impact of text correction.

- **Error Analysis**: Tools (`calculate_levenshtein_distance.py`) to analyze and compare the characteristics of synthetic noise against naturally occurring typos.

## Installation

### Clone the repository:

```bash
git clone https://github.com/ahmadmwali/HausaSeq2Seq.git
cd HausaSeq2Seq
```

### Create a virtual environment (recommended):

```bash
python3 -m venv venv
source venv/bin/activate
```

### Install dependencies:

The project relies on several Python packages. Install them using the `requirements.txt` file.

```bash
pip install -r requirements.txt
```

## Data Access

The files used for training, validation, testing, and downstream evaluation are available for download.

**Download Link**: https://drive.google.com/drive/folders/1dZxaEQXA3yBP4aiWX5ZeXF3ciN-0OSEn?usp=sharing

Please download the data and place it in a `Files/` directory at the root of the project, or update the file paths in the scripts accordingly.

> **Note:** The Google Drive link provides access to all datasets used in this project, including the processed outputs generated by `get_data.py` (`train.tsv`, `validation.tsv`, and `test.tsv`), as well as supplementary data used during training and evaluation.


## Usage Guide

The following sections detail how to use the scripts to replicate the research pipeline.

### 1. Data Preparation (`get_data.py`)

This script processes raw text files, segments them into sentences, generates synthetic noise, and creates the final `train.tsv`, `validation.tsv`, and `test.tsv` datasets.

#### Basic Usage (with default noise):

This command processes input files, excludes default religious texts, and uses differential noising probabilities for frequent vs. infrequent words.

```bash
python get_data.py \
    --input_files ./Files/file1.txt ./Files/file2.txt \
    --train_output ./data/train.tsv \
    --val_output ./data/validation.tsv \
    --test_output ./data/test.tsv \
    --train_size 440000 \
    --val_size 5000 \
    --test_size 5000 \
    --top_natural_words_path ./top_100_hausa_natural.txt \
    --prob_noise_frequent_word 0.02 \
    --prob_noise_other_word 0.4
```

#### Usage with Custom Noise (from JSON file):

You can define custom character-level noise probabilities in a JSON file (`noise_config.json`).

```json
{
    "random_spacing": 0.02,
    "remove_spaces": 0.15,
    "incorrect_characters": 0.02,
    "delete_characters": 0.005,
    "duplicate_characters": 0.01,
    "substitute_characters": 0.001,
    "transpose_characters": 0.01,
    "delete_chunk": 0.0015,
    "insert_chunk": 0.001
}
```

Then run the script:

```bash
python get_data.py \
    --input_files ./Files/*.txt \
    --train_output ./data/train.tsv \
    --val_output ./data/validation.tsv \
    --test_output ./data/test.tsv \
    --noise_config ./noise_config.json \
    --top_natural_words_path ./top_100_hausa_natural.txt
```

### 2. Model Training (`train.py`)

Use this script to fine-tune a transformer model on the data generated in the previous step.

#### Full Fine-tuning Example (T5 model):

```bash
python train.py \
    --model_type t5 \
    --base_model_name castorini/afriteva_small \
    --train_file ./data/train.tsv \
    --val_file ./data/validation.tsv \
    --output_dir_base ./results/t5_full_finetune \
    --learning_rate 3e-5 \
    --train_batch_size 8 \
    --eval_batch_size 16 \
    --num_epochs 3 \
    --max_seq_length 128
```

#### LoRA Fine-tuning Example (M2M100 model):

This example uses LoRA for parameter-efficient fine-tuning and pushes the final adapter to the Hugging Face Hub.

```bash
python train.py \
    --model_type bart \
    --base_model_name facebook/m2m100_418M \
    --use_lora \
    --push_to_hub \
    --hub_model_id_prefix your-username/m2m100-ha-lora-corrector \
    --hf_token "hf_YOUR_API_TOKEN" \
    --train_file ./data/train.tsv \
    --val_file ./data/validation.tsv \
    --source_lang_code ha \
    --target_lang_code ha \
    --lora_r 8 \
    --lora_alpha 16 \
    --lora_learning_rate 5e-4 \
    --train_batch_size 4 \
    --gradient_accumulation_steps 8 \
    --num_epochs 5
```

### 3. Model Testing (`test.py`)

Evaluate your trained model on the held-out test set to get performance metrics.

#### Testing a Full Fine-tuned Model:

```bash
python test.py \
    --model_type t5 \
    --hub_model_id your-username/harmonized-afriteva_small-full \
    --test_file ./data/test.tsv \
    --results_dir_base ./test_results/t5_full \
    --eval_batch_size 16
```

#### Testing a LoRA Adapter:

```bash
python test.py \
    --model_type bart \
    --hub_model_id your-username/m2m100-ha-lora-corrector \
    --is_lora_adapter \
    --base_model_name_for_lora facebook/m2m100_418M \
    --test_file ./data/test.tsv \
    --results_dir_base ./test_results/m2m100_lora \
    --source_lang_code ha \
    --target_lang_code ha
```

### 4. Running Predictions (`predict_on_dataframe.py`)

Use a trained model to correct text in any TSV or CSV file.

```bash
python predict_on_dataframe.py \
    --model_type t5 \
    --hub_model_id ahmadmwali/m2m_trial2 \
    --input_file ./data/unseen_noisy_data.tsv \
    --output_file ./results/predictions.tsv \
    --input_text_column 'hausa_text' \
    --prediction_column_name 'corrected_text' \
    --prediction_batch_size 16
```

### 5. Downstream Task Evaluation (`evaluate_downstream.py`)

This script trains a classifier on both original and corrected text to measure the impact of the correction model.

```bash
python evaluate_downstream.py \
    --file-path ./data/hausa_sentiment_dataset.tsv \
    --tweet-col 'raw_tweet' \
    --cleaned-col 'corrected_tweet' \
    --label-col 'sentiment' \
    --model-checkpoint "bert-base-multilingual-cased" \
    --output-dir ./sentiment_evaluation_results \
    --num-epochs 3 \
    --batch-size 16
```

### 6. Levenshtein Distance Analysis (`calculate_levenshtein_distance.py`)

Compare the characteristics of a natural (e.g., Twitter) dataset vs. your synthetically generated dataset.

```bash
python calculate_levenshtein_distance.py \
    --natural_file ./data/twitter_hausa.tsv \
    --synthetic_file ./data/train.tsv \
    --natural_text_col 'tweet' \
    --synthetic_text_col 'Noisy' \
    --output_dir ./analysis_plots
```

## Script Descriptions

- **`get_data.py`**: Pre-processes raw text, generates synthetic noise, and creates train/validation/test splits.

- **`train.py`**: Fine-tunes a sequence-to-sequence transformer model using the generated data. Supports full and LoRA fine-tuning.

- **`test.py`**: Evaluates a trained model on the test set using BLEU, WER, CER, F1, and METEOR metrics.

- **`predict_on_dataframe.py`**: Applies a trained model to an arbitrary TSV/CSV file to generate corrections.

- **`evaluate_downstream.py`**: Measures the impact of text correction on a downstream classification task.

- **`calculate_levenshtein_distance.py`**: Analyzes and visualizes the distribution of Levenshtein distances and typo cluster sizes to compare natural vs. synthetic errors.

- **`helper_functions.py`**: Contains utility functions for metrics calculation and data preprocessing used by other scripts.

- **`requirements.txt`**: A list of all Python package dependencies for the project.

## Configuration

The noise generation process in `get_data.py` is highly configurable. You can control the probability of noising frequent vs. infrequent words and the application rate of various character-level corruptions.

- `--prob_noise_frequent_word`: Probability of applying noise to a word found in the `--top_natural_words_path` list.

- `--prob_noise_other_word`: Probability of applying noise to any other word.

- `--noise_config`: A JSON string or file path specifying the probabilities for each character-level noise function (e.g., `delete_characters`, `transpose_characters`).

## Model Information

This project was tested with several multilingual and African-language-focused transformer models, including:

- M2M100 (`facebook/m2m100_418M`)
- mBART (`facebook/mbart-large-50-many-to-many-mmt`)
- AfriTEVA (`castorini/afriteva_small`)

## Citation

If you use this work, please cite the original paper:

```bibtex
@misc{wali2025automaticcorrectionwritinganomalies,
      title={Automatic Correction of Writing Anomalies in Hausa Texts}, 
      author={Ahmad Mustapha Wali and Sergiu Nisioi},
      year={2025},
      eprint={2506.03820},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2506.03820}, 
}
```

## License

This project is licensed under the Creative Commons Attribution 4.0 International License (CC-BY-4.0). See the `LICENSE.md` file for details.

## Contact & Contributions

For questions, issues, or suggestions, please open an issue in this repository. We welcome contributions to improve the project!
